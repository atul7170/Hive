Scenario Based questions:

Will the reducer work or not if you use “Limit 1” in any HiveQL query?
--Yes, using "LIMIT 1" in a HiveQL query will work with a reducer. The reducer is responsible for reducing the output from the mappers, and it will still function properly even if only one row is returned due to the "LIMIT 1" clause.

Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time?
--If multiple clients try to access Hive simultaneously with the default metastore configuration, they will all connect to the same metastore database. This can lead to potential conflicts and concurrency issues, such as data inconsistency or performance degradation, as the clients might interfere with each other's operations or overwrite each other's changes.

Suppose, I create a table that contains details of all the transactions done by the customers: 
CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?
--To improve the query performance and optimize the processing time in Hive, you can follow these steps:
1.Partition the table
2.Use appropriate file formats:Parquet and ORC (Optimized Row Columnar) formats are commonly used in Hive.
3.Apply bucketing

How can you add a new partition for the month December in the above partitioned table?
--ALTER TABLE transaction_details ADD PARTITION (month='December');

I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?
--Option 1: Disable dynamic partition strict mode
SET hive.exec.dynamic.partition.mode=nonstrict;
Option 2: Add a static partition column
ALTER TABLE transaction_details ADD COLUMNS (country STRING);
INSERT INTO TABLE transaction_details PARTITION (country='your_country_value') VALUES (...);

Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?
--CREATE EXTERNAL TABLE sample_table (
  id INT,
  first_name STRING,
  last_name STRING,
  email STRING,
  gender STRING,
  ip_address STRING
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
  "separatorChar" = ",",
  "quoteChar" = "\"",
  "escapeChar" = "\\"
)
LOCATION '/temp'
TBLPROPERTIES ("skip.header.line.count"="1");


Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?
--1.Use Hadoop Archive (HAR): Create a Hadoop Archive (HAR) file from the combined CSV files. The HAR file is a compressed archive that stores multiple files in a single container, reducing the overhead of handling a large number of small files.

2.Move the HAR file to HDFS: Move the generated HAR file to HDFS, preferably to a separate directory dedicated to input data.

3.Create an external table in Hive: Create an external table in Hive that points to the location of the HAR file. Use the appropriate schema to match the data format of the CSV files. For example:
CREATE EXTERNAL TABLE csv_data (
  id INT,
  name STRING,
  email STRING,
  country STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION 'hdfs://your_hdfs_path_to_har_file';



LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;

The following statement failed to execute. What can be the cause?
--Incorrect path
--Missing file or directory
--File format or structure


Hive Practical questions:

Hive Join operations

Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT
)

Now perform different joins operations on top of these tables
(Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)
-hive> CREATE TABLE CUSTOMERS (
    >   ID INT,
    >   NAME STRING,
    >   AGE INT,
    >   ADDRESS STRING,
    >   SALARY FLOAT
    > );
	
hive> CREATE TABLE ORDERS (
    > OID INT,
    > DAATE STRING,
    > CUSTOMER_ID INT,
    > AMOUNT FLOAT
    > );
select * from CUSTOMERS join ORDERS on CUSTOMERS.ID=ORDERS.CUSTOMER_ID;






BUILD A DATA PIPELINE WITH HIVE



--LOAD DATA INPATH '/AirQualityUCI.csv' INTO TABLE AirQ;

1. Create a hive table as per given schema in your dataset 
--CREATE TABLE AirQ (Datee string
,Timee string
,CO INT
,PT08S1 INT
,NMHC INT
,C6H6 INT
,PT08S2 INT
,NOx INT
,PT08S3 INT
,NO2 INT
,PT08S4 INT
,PT08S5 INT
,T INT
,RH INT
,AH INT
);

2. try to place a data into table location

--hdfs dfs -put AirQualityUCI.csv /

3. Perform a select operation . 
--hive>select * from AirQ;

4. Fetch the result of the select operation in your local as a csv file . 
hive> INSERT OVERWRITE DIRECTORY '/'
    > ROW FORMAT DELIMITED
    > FIELDS TERMINATED BY ','
    > SELECT *
    > FROM AirQ;
	
5. Perform group by operation . 
>select Datee,Sum(CO) from AirQ group by Datee;
